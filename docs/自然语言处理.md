[TOC]

# 自然语言处理

## 列出几种文本特征提取算法

**词袋模型**:  \
__文档频率__ \
**信息增益**：信息增益用于选择哪些词汇应该作为特征来输入模型，而不是直接用作特征向量的值。通过计算信息增益，我们可以确定哪些词汇在分类任务中最有帮助，然后在特征向量中使用这些词汇来表示文档.
  先计算整个数据集的熵，减去每个单词的熵便是每个单词的信息增益，最后选出信息增益高的几个单词组成特征向量。 \
**互信息**:
、X^2统计、TF-IDF

## RNN

### 基本原理

RNN（循环神经网络）的基本原理是在序列数据的处理上引入了“记忆”机制。与传统的前馈神经网络不同，RNN的隐藏层不仅接收当前时刻的输入，还接收上一时刻隐藏层的输出（即前一时刻的状态）。这种结构使得RNN能够保留序列数据中的时间依赖关系。

### RNN 常见的几种设计模式是怎样的？

1. 一对一：非序列任务，常用于传统分类问题
2. 一对多：生成任务，如图像描述生成
3. 多对一：序列分析任务，如情感分析
4. 多对多（等长序列）：逐步标注或分类，如视频处理或序列标注
5. 多对多（不等长序列）：序列到学列任务，如机器翻译 ——需要编码器

### RNN 为什么会梯度消失？

根本原因在于 反向传播时梯度的反复相乘，特别是当序列很长时，RNN会对每个时间步进行反向传播，而这个过程中梯度在多次传递时可能逐渐变小，从而导致梯度消失。

在使用tanh(x)和sigmoid(x)作为激活函数时，当在其饱和区域时（即tanh输出接近1或-1，sigmoid输出接近1或0时），导数的值趋近于0。容易梯度消失。

解决方案：使用LSTM，GRU；合适的权重初始化方式（Xavier初始化：保证输入和输出的方差相等）

### RNN 为什么会梯度爆炸？

同上一题，如果正好处在非饱和区间，激活函数导数值会较大，加上时间步的累积效应，反向传播时梯度会快速增长，出现爆炸。

解决方案：梯度裁剪；权重初始化；正则化；优化算法

### RNN中为什么要采用tanh而不是ReLu作为激活函数？

 RNN 处理的是 时间序列数据，而 tanh 和 sigmoid 这类平滑的、饱和型的非线性激活函数更适合处理隐藏状态的变化和序列信息的传递。
 ReLU可能导致梯度爆炸
 ReLU，如果某些时间步的隐藏状态为负值，使用 ReLU 会将其输出置为 0，导致信息丢失。长此以往，可能会出现许多隐藏单元始终不再激活，导致模型的表达能力下降。

### RNN和CNN对比，RNN对文本的时间序列的优点。

能够捕捉输入序列中的时间依赖性和上下文关系。在文本处理中，每个词在句子中的含义往往依赖于前后词的上下文关系。RNN通过其循环结构，可以有效地建模这种时间序列上的依赖关系，尤其是对长句子或长序列的处理。
CNN在处理文本时，虽然可以通过卷积核捕捉局部模式，但它的卷积操作通常是并行进行的，并不会直接考虑序列的时间顺序。CNN更适合处理局部模式。

## LSTM

### LSTM 基本原理

通过引入“门控机制”来控制信息的流动，从而有效地捕捉长期依赖关系。LSTM的核心思想是通过遗忘门、输入们、输出门来决定哪些信息保留、哪些信息更新。

### LSTM 怎么能解决梯度消失问题？

LSTM缓解梯度消失的核心原因在于细胞状态的加性结构，而不仅仅是遗忘门的乘法。
C_t = f_t \dot C_{t-1} + i_t \dot \flash{C}_t
相比于传统RNN中的乘法操作（h_t = tanh(W \dot h_{t-1})，加法使得梯度可以沿着时间步线性地传递，使得梯度的流动更加稳定。

### LSTM 用来解决RNN的什么问题？

1. 梯度消失和梯度爆炸
2. 长距离依赖问题：RNN对近期的记忆比较友好，随着时间步的推移，早期输入会被逐渐遗忘。

### LSTM 和 GRU 区别

__结构上__：LSTM有遗忘门、输入门和输出门；GRU只有更新门和重置门。
__复杂度__：LSTM计算更复杂，但能更好处理长期依赖；GRU简化了结构，计算上更为简单。

### RNN、LSTM 和 GRU公式与结构图


## 深度学习如何提取query特征？

基于静态词嵌入：Word2Vec、Glove \
基于上下文：ELMo \
基于预训练的Transformer：BERT、RoBERTa、ALBERT

### 如何比较文本的相似度？

1. 基于词或字符的相似度
- 编辑距离
- Jaccard相似系数
- Cosine相似度
2. 基于词嵌入的相似度
- Word2Vec
- GloVe
3. 基于上下文嵌入的相似度
-ELMo

- 协同过滤相似度
- 余弦相似度
- tf-idf相似度
- 深度学习词向量

## 如何利用深度学习计算语义相似度？

- [ ] TODO

## word2vec

### word2vec 基本原理

通过一个神经网络模型，将词嵌入到一个低维向量空间，使得语义相近的词在向量空间中彼此接近。Word2Vec 主要有两种训练模型：CBOW（Continuous Bag of Words） 和 Skip-Gram。

CBOW：给定目标词周围的上下文词，预测目标词的可能性。例如，在句子“猫在桌子上睡觉”中，CBOW 通过观察“猫”、“在”、“桌子”、“睡觉”这些上下文词来预测“上”这个目标词。

Skip-Gram：目标是通过给定一个中心词，来预测它的上下文词。例如，给定“上”这个词，预测“猫”、“在”、“桌子”、“睡觉”等上下文词。

### 使用哈夫曼树的原因是什么？

对于词汇量非常大的词汇表，使用普通的 softmax 会导致计算成本非常高。哈夫曼树的使用有助于高效地计算目标词的概率。\
哈夫曼树是一棵 带权重的最优二叉树，其构造基于词的频率（出现次数）。在哈夫曼树中，出现频率较高的词会离根节点更近，路径较短；出现频率较低的词会离根节点较远，路径较长。

### word2vec的输入输出层是什么样的？

输入层：通常是独热编码，隐藏层将输入词转换为一个低维稠密向量 \
输出层：通过概率分布预测目标词。

### word2vec 里面的层次索引

即哈夫曼树

### word2vec如何训练的？

- [ ] TODO

### 如何判断 word2vec 的效果好坏？

定性：词语相似性，类比测试，可视化（PCA降维）
定量：词语相似性任务，下游表现任务，聚类任务

### 介绍一下 Word2vec，CBOW和Skip-gram的区别是什么？

- [ ] TODO

### word2vec 和 tf-idf 相似度计算时的区别？

Word2Vec 是一种基于神经网络的词向量模型，它通过上下文信息来学习每个词的低维稠密向量表示。通过训练，语义相近的词会在向量空间中彼此靠近。因此，Word2Vec 能够捕捉到词语的语义关系和上下文信息。\

由于每个词有一个向量表示，整个句子或文档的向量可以通过对每个词的词向量取平均或加权平均来得到。然后使用余弦相似度计算两个文档向量之间的相似度 \

TF-IDF 是基于词频的统计模型。TF（词频）表示某个词在文档中出现的次数，IDF（逆文档频率）表示该词在整个语料库中的稀有程度。通过 TF 和 IDF 的乘积，衡量一个词在某篇文档中的重要性。 \

TF-IDF是通过对整个文档进行的向量化，比如 【我 喜欢 吃 苹果】被向量化为 【0,0,0，0.173，0,0],另一个文档被向量化为另一个等长向量，向量长度是词汇表长度，再进行余弦相似度计算。

### word2vec 和 NNLM 对比有什么区别？（word2vec vs NNLM）

- [ ] TODO

### word2vec 负采样有什么作用？

负采样（Negative Sampling） 被提出作为 Softmax 的一种近似方法。其核心思想是，在训练过程中，只更新目标词和少量负样本的权重，而不再对整个词汇表进行计算。正常的是先通过softmax计算所有的概率，通过交叉熵计算概率函数；而负采样是通过叠加损失函数，使正样本与目标词的相似度更大，使负样本（随机选择的假邻居词）与目标词的相似度更小构建新的损失函数。

负采样这个点引入word2vec非常巧妙，两个作用，1.加速了模型计算，2.保证了模型训练的效果，一个是模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点非常聪明。

### word2vec 和 fastText对比有什么区别？（word2vec vs fastText）

- [ ] TODO

## GRU 和 LSTM、RNN的区别是什么？

- [ ] TODO

## Sentence Embedding

- [ ] TODO

## SeqSeq

- [ ] TODO

### seq2seq-attention原理和公式

- [ ] TODO

## Transformer 

### 基本原理

- [ ] TODO

### transformer 各部分怎么用？Q K V怎么计算；Attention怎么用？

- [ ] TODO

### 详细说一下 transformer 的 encoder 过程

- [ ] TODO

- [Transformer面试题总结101道题](https://zhuanlan.zhihu.com/p/438625445)
- [transformer面试题的简单回答](https://zhuanlan.zhihu.com/p/363466672)
- [史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！](https://zhuanlan.zhihu.com/p/148656446)

## 如何做文本摘要？

- [ ] TODO

## CTC loss公式推导

- [ ] TODO

## ELMO

- [ ] TODO

## BERT

### 基本原理

- [ ] TODO

### elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）

- [ ] TODO

## XLNet

### 基本原理

- [ ] TODO

## 生成式问答解决生成句子多样性的方法

- [ ] TODO

## 怎么评价生成效果的好坏？

- [ ] TODO

## 参考资料

- [2018-暑期实习生-自然语言处理算法岗-面试题](<https://blog.csdn.net/qq_28031525/article/details/80028055>)

- [NLP Interview Questions](https://medium.com/modern-nlp/nlp-interview-questions-f062040f32f7)  [百度云链接](https://pan.baidu.com/s/1fY8HXiswGA1rbCnuGG5TXA)  提取码：h9k8 

- https://github.com/songyingxin/NLPer-Interview

- [Transformer面试题总结101道题](https://zhuanlan.zhihu.com/p/438625445)

- [transformer面试题的简单回答](https://zhuanlan.zhihu.com/p/363466672)

- [史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！](https://zhuanlan.zhihu.com/p/148656446)

  
  
