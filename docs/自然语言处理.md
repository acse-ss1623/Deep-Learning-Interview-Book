[TOC]

# 自然语言处理

## 列出几种文本特征提取算法

**词袋模型**:  \
__文档频率__ \
**信息增益**：信息增益用于选择哪些词汇应该作为特征来输入模型，而不是直接用作特征向量的值。通过计算信息增益，我们可以确定哪些词汇在分类任务中最有帮助，然后在特征向量中使用这些词汇来表示文档.
  先计算整个数据集的熵，减去每个单词的熵便是每个单词的信息增益，最后选出信息增益高的几个单词组成特征向量。 \
**互信息**:
、X^2统计、TF-IDF

## RNN

### 基本原理

RNN（循环神经网络）的基本原理是在序列数据的处理上引入了“记忆”机制。与传统的前馈神经网络不同，RNN的隐藏层不仅接收当前时刻的输入，还接收上一时刻隐藏层的输出（即前一时刻的状态）。这种结构使得RNN能够保留序列数据中的时间依赖关系。

### RNN 常见的几种设计模式是怎样的？

1. 一对一：非序列任务，常用于传统分类问题
2. 一对多：生成任务，如图像描述生成
3. 多对一：序列分析任务，如情感分析
4. 多对多（等长序列）：逐步标注或分类，如视频处理或序列标注
5. 多对多（不等长序列）：序列到学列任务，如机器翻译 ——需要编码器

### RNN 为什么会梯度消失？

根本原因在于 反向传播时梯度的反复相乘，特别是当序列很长时，RNN会对每个时间步进行反向传播，而这个过程中梯度在多次传递时可能逐渐变小，从而导致梯度消失。

在使用tanh(x)和sigmoid(x)作为激活函数时，当在其饱和区域时（即tanh输出接近1或-1，sigmoid输出接近1或0时），导数的值趋近于0。容易梯度消失。

解决方案：使用LSTM，GRU；合适的权重初始化方式（Xavier初始化：保证输入和输出的方差相等）

### RNN 为什么会梯度爆炸？

同上一题，如果正好处在非饱和区间，激活函数导数值会较大，加上时间步的累积效应，反向传播时梯度会快速增长，出现爆炸。

解决方案：梯度裁剪；权重初始化；正则化；优化算法

### RNN中为什么要采用tanh而不是ReLu作为激活函数？

 RNN 处理的是 时间序列数据，而 tanh 和 sigmoid 这类平滑的、饱和型的非线性激活函数更适合处理隐藏状态的变化和序列信息的传递。
 ReLU可能导致梯度爆炸
 ReLU，如果某些时间步的隐藏状态为负值，使用 ReLU 会将其输出置为 0，导致信息丢失。长此以往，可能会出现许多隐藏单元始终不再激活，导致模型的表达能力下降。

### RNN和CNN对比，RNN对文本的时间序列的优点。

能够捕捉输入序列中的时间依赖性和上下文关系。在文本处理中，每个词在句子中的含义往往依赖于前后词的上下文关系。RNN通过其循环结构，可以有效地建模这种时间序列上的依赖关系，尤其是对长句子或长序列的处理。
CNN在处理文本时，虽然可以通过卷积核捕捉局部模式，但它的卷积操作通常是并行进行的，并不会直接考虑序列的时间顺序。CNN更适合处理局部模式。

## LSTM

### LSTM 基本原理

通过引入“门控机制”来控制信息的流动，从而有效地捕捉长期依赖关系。LSTM的核心思想是通过遗忘门、输入们、输出门来决定哪些信息保留、哪些信息更新。

### LSTM 怎么能解决梯度消失问题？

LSTM缓解梯度消失的核心原因在于细胞状态的加性结构，而不仅仅是遗忘门的乘法。
C_t = f_t \dot C_{t-1} + i_t \dot \flash{C}_t
相比于传统RNN中的乘法操作（h_t = tanh(W \dot h_{t-1})，加法使得梯度可以沿着时间步线性地传递，使得梯度的流动更加稳定。

### LSTM 用来解决RNN的什么问题？

1. 梯度消失和梯度爆炸
2. 长距离依赖问题：RNN对近期的记忆比较友好，随着时间步的推移，早期输入会被逐渐遗忘。

### LSTM 和 GRU 区别

__结构上__：LSTM有遗忘门、输入门和输出门；GRU只有更新门和重置门。
__复杂度__：LSTM计算更复杂，但能更好处理长期依赖；GRU简化了结构，计算上更为简单。

### RNN、LSTM 和 GRU公式与结构图



## 深度学习如何提取query特征？

- [ ] TODO

### 如何比较文本的相似度？

- 协同过滤相似度
- 余弦相似度
- tf-idf相似度
- 深度学习词向量

## 如何利用深度学习计算语义相似度？

- [ ] TODO

## word2vec

### word2vec 基本原理

- [ ] TODO

### 使用哈夫曼树的原因是什么？

- [ ] TODO

### word2vec的输入输出层是什么样的？

- [ ] TODO

### word2vec 里面的层次索引

- [ ] TODO

### word2vec如何训练的？

- [ ] TODO

### 如何判断 word2vec 的效果好坏？

- [ ] TODO

### 介绍一下 Word2vec，CBOW和Skip-gram的区别是什么？

- [ ] TODO

### word2vec 和 tf-idf 相似度计算时的区别？

- [ ] TODO

### word2vec 和 NNLM 对比有什么区别？（word2vec vs NNLM）

- [ ] TODO

### word2vec 负采样有什么作用？

负采样这个点引入word2vec非常巧妙，两个作用，1.加速了模型计算，2.保证了模型训练的效果，一个是模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点非常聪明。

### word2vec 和 fastText对比有什么区别？（word2vec vs fastText）

- [ ] TODO

## GRU 和 LSTM、RNN的区别是什么？

- [ ] TODO

## Sentence Embedding

- [ ] TODO

## SeqSeq

- [ ] TODO

### seq2seq-attention原理和公式

- [ ] TODO

## Transformer 

### 基本原理

- [ ] TODO

### transformer 各部分怎么用？Q K V怎么计算；Attention怎么用？

- [ ] TODO

### 详细说一下 transformer 的 encoder 过程

- [ ] TODO

- [Transformer面试题总结101道题](https://zhuanlan.zhihu.com/p/438625445)
- [transformer面试题的简单回答](https://zhuanlan.zhihu.com/p/363466672)
- [史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！](https://zhuanlan.zhihu.com/p/148656446)

## 如何做文本摘要？

- [ ] TODO

## CTC loss公式推导

- [ ] TODO

## ELMO

- [ ] TODO

## BERT

### 基本原理

- [ ] TODO

### elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）

- [ ] TODO

## XLNet

### 基本原理

- [ ] TODO

## 生成式问答解决生成句子多样性的方法

- [ ] TODO

## 怎么评价生成效果的好坏？

- [ ] TODO

## 参考资料

- [2018-暑期实习生-自然语言处理算法岗-面试题](<https://blog.csdn.net/qq_28031525/article/details/80028055>)

- [NLP Interview Questions](https://medium.com/modern-nlp/nlp-interview-questions-f062040f32f7)  [百度云链接](https://pan.baidu.com/s/1fY8HXiswGA1rbCnuGG5TXA)  提取码：h9k8 

- https://github.com/songyingxin/NLPer-Interview

- [Transformer面试题总结101道题](https://zhuanlan.zhihu.com/p/438625445)

- [transformer面试题的简单回答](https://zhuanlan.zhihu.com/p/363466672)

- [史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！](https://zhuanlan.zhihu.com/p/148656446)

  
  
